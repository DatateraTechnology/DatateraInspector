# -*- coding: utf-8 -*-
"""Sensitive_Data_Detection_Image.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B78LyfMgjySVE44_fFO4yUgp6ztYPYHv

# **SENSITIVE IMAGE DATA INDICATOR**

# This notebook aims to capture Sensitive Data (Name, email address, password, phone number, date of birth etc) for a given image data.
Model usus Pytesseract for text extraction from an image data, For sensititive data detection, model uses Presidio Analyzer and Presidio Anonymizer. Anonymizing data is optional. Name Entity Recogntion (NER) models are pretrained models and they don't need to be trained. Entities such CREDIT_CARD, IBAN_CODE, EMAIL_ADDRESS, US_BANK_NUMBER can be detected and anonymized(optional) with this model. Output gives detection of sensitive data for chosen entities.

Importing Libraries
"""

import numpy as np
from matplotlib import pyplot as plt
import pytesseract
from pytesseract import Output
import shutil
import os
import random
try:
    from PIL import Image
except ImportError:
    import Image

from PIL import Image
import re
import cv2

import spacy
from typing import List
import pprint
from presidio_analyzer import AnalyzerEngine, PatternRecognizer, EntityRecognizer, Pattern, RecognizerResult
from presidio_analyzer.recognizer_registry import RecognizerRegistry
from presidio_analyzer.nlp_engine import NlpEngine, SpacyNlpEngine, NlpArtifacts
from presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer
from presidio_anonymizer import AnonymizerEngine
from transformers import pipeline
import warnings
from os import path
import json
import io
import skimage
from azure.storage.blob import BlobServiceClient

import requests

warnings.filterwarnings("ignore")

config = path.relpath("config/app-config.json")
img_path = path.relpath("images/im.jpg")

with open(config, "r") as f:
    config = json.load(f)
def getImageSensitiveData(path):
    #pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

    r = requests.get(path)
    stream = io.BytesIO(r.content)
    with open(img_path, "wb") as f:
        f.write(stream.getbuffer())

    #img = Image.open(img_path)

    ############################
    img = Image.open(img_path)
    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)


    ##############################
    #print(img)

    scale_factor = 1600.0 / np.max(img.shape[:2])
    img = cv2.resize(img, (0, 0), fx=scale_factor, fy=scale_factor)

    df = pytesseract.image_to_data(
        255 - img, lang="eng", config="--psm 6", output_type=pytesseract.Output.DATAFRAME
    )

    #print(df)
    # group recognized words by lines
    for line_num, gdf in df.groupby("line_num"):
        line_confidence = gdf["conf"].mean()
        if line_confidence < 30:
            continue

        gdf = gdf[gdf["conf"] >= 90]
        if not len(gdf):
            continue



    text1 = []
    for line_num, gdf in df.groupby("line_num"):
        line_confidence = gdf["conf"].mean()
        if line_confidence < 30:
            continue

        gdf = gdf[gdf["conf"] >= 90]
        if not len(gdf):
            continue

        text1.append(gdf["text"].values)

    text1 = "".join(map(str, text1)).capitalize().strip()

    for char in text1:
        if char in " ?.!/;:":
            text1.replace(char, '')

    """# **#Model (For Datasets and text (sentence like) data)**

    **Advanced Sensitive Data Detection and Anonymization(Optional) by using Hugging Face Transformers**

    Presidio (The Presidio analyzer is a Python based service for detecting Sensitive Data in text) helps to ensure sensitive data is properly managed and governed. It provides fast identification and anonymization modules for private entities in text such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more.

    By Default Presidio is using Spacy for Sensitive Data detection and extraction. In this model are we going to replace spacy with a Hugging Face Transformer to perform detection and anonymization. Presidio supports already out of the box 24 PII entities including, CREDIT_CARD, IBAN_CODE, EMAIL_ADDRESS, US_BANK_NUMBER, US_ITIN.

    Creating Model
    """

    # load spacy model -> workaround
    #os.system("spacy download en_core_web_lg")

    # list of entities: https://microsoft.github.io/presidio/supported_entities/#list-of-supported-entities
    DEFAULT_ANOYNM_ENTITIES = [
        "CREDIT_CARD",
        "CRYPTO",
        "DATE_TIME",
        "EMAIL_ADDRESS",
        "IBAN_CODE",
        "IP_ADDRESS",
        "NRP",
        "LOCATION",
        "PERSON",
        "PHONE_NUMBER",
        "MEDICAL_LICENSE",
        "URL",
        "US_SSN", "US_BANK_NUMBER"
    ]

    # init anonymize engine
    engine = AnonymizerEngine()
    class HFTransformersRecognizer(EntityRecognizer):
        def __init__(
                self,
                model_id_or_path=None,
                aggregation_strategy="simple",
                supported_language="en",
                ignore_labels=["O", "MISC"],
        ):
            # inits transformers pipeline for given mode or path
            self.pipeline = pipeline(
                "token-classification", model=model_id_or_path, aggregation_strategy=aggregation_strategy,
                ignore_labels=ignore_labels
            )
            # map labels to presidio labels
            self.label2presidio = {
                "PER": "PERSON",
                "LOC": "LOCATION",
                "ORG": "ORGANIZATION",
            }

            # passes entities from model into parent class
            super().__init__(supported_entities=list(self.label2presidio.values()),
                             supported_language=supported_language)
        def load(self) -> None:
            """No loading is required."""
            pass

        def analyze(
                self, text: str, entities: List[str] = None, nlp_artifacts: NlpArtifacts = None
        ) -> List[RecognizerResult]:
            """
            Extracts entities using Transformers pipeline
            """
            results = []

            # keep max sequence length in mind
            predicted_entities = self.pipeline(text)
            if len(predicted_entities) > 0:
                for e in predicted_entities:
                    converted_entity = self.label2presidio[e["entity_group"]]
                    if converted_entity in entities or entities is None:
                        results.append(
                            RecognizerResult(
                                entity_type=converted_entity, start=e["start"], end=e["end"], score=e["score"]
                            )
                        )
            return results

    def model_fn(model_dir):
        transformers_recognizer = HFTransformersRecognizer(model_dir)
        # Set up the engine, loads the NLP module (spaCy model by default) and other PII recognizers
        analyzer = AnalyzerEngine()
        analyzer.registry.add_recognizer(transformers_recognizer)
        return analyzer

    def predict_fn(data, analyzer):
        sentences = data.pop("inputs", data)

        if "parameters" in data:
            anonymization_entities = data["parameters"].get("entities", DEFAULT_ANOYNM_ENTITIES)
            anonymize_text = data["parameters"].get("anonymize", False)
        else:
            anonymization_entities = DEFAULT_ANOYNM_ENTITIES
            anonymize_text = False


        # identify entities
        results = analyzer.analyze(text=sentences, entities=anonymization_entities, language="en")
        # anonymize text
        if anonymize_text:
            result = engine.anonymize(text=sentences, analyzer_results=results)
            return {"anonymized": result.text}

        os.remove(img_path)

        return {"found": [entity.to_dict() for entity in results]}


    """After building the model we will look into the text data extracted from image and analyze it with the help of the model

    Predicting second image for sensitive data
    """

    """Predicting first image for sensitive data """

    data = {
        "inputs": text1
    }

    return predict_fn(data, AnalyzerEngine())
